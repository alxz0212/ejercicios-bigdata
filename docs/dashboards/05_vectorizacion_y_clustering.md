# üéì Gu√≠a Formativa: Vectorizaci√≥n y Clustering de Documentos

## üöÄ An√°lisis de T√≥picos con Inteligencia Artificial (NLP & Machine Learning)

---

> ### üìù Informaci√≥n de Certificaci√≥n y Referencia
>
> **Autor original/Referencia:** @TodoEconometria  
> **Profesor:** Juan Marcelo Gutierrez Miranda  
> **Metodolog√≠a:** Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA & Econometr√≠a Aplicada.  
> **Hash ID de Certificaci√≥n:** `4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c`  
> **Repositorio:** [https://github.com/TodoEconometria/certificaciones](https://github.com/TodoEconometria/certificaciones)  
>
> **REFERENCIA ACAD√âMICA:**
>
> - McKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media.
> - Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362.
> - Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.

---

## üèõÔ∏è 1. Introducci√≥n al Laboratorio

En el mundo del **Big Data**, la mayor parte de la informaci√≥n es **no estructurada** (textos, correos, noticias). El reto fundamental es: *¬øC√≥mo puede una computadora entender que dos documentos hablan de lo mismo sin leerlos?*

Este ejercicio implementa un pipeline completo de Ciencia de Datos para agrupar autom√°ticamente **1,200 documentos** en espa√±ol, utilizando una combinaci√≥n de t√©cnicas matem√°ticas avanzadas para transformar el lenguaje humano en estructuras que las m√°quinas pueden procesar.

---

## üß† 2. Fundamentos Te√≥ricos (¬øQu√© usamos y por qu√©?)

### A. Vectorizaci√≥n: El Puente entre Texto y Matem√°ticas

Las computadoras no entienden palabras, solo n√∫meros. Usamos **TF-IDF (Term Frequency - Inverse Document Frequency)** por su capacidad de discernir la relevancia.

- **¬øQu√© es?**: Un valor estad√≠stico que busca medir qu√© tan importante es una palabra para un documento dentro de una colecci√≥n (corpus).
- **¬øC√≥mo funciona?**:
    $$TF(t, d) = \frac{\text{Conteo de la palabra } t \text{ en documento } d}{\text{Total de palabras en } d}$$
    $$IDF(t) = \log\left(\frac{\text{Total de documentos}}{\text{Documentos que contienen la palabra } t}\right)$$
- **¬øPor qu√© lo usamos?**: A diferencia del simple conteo (Bak-of-Words), el TF-IDF penaliza palabras que aparecen en todos lados (como "el", "que", "es") y premia palabras tem√°ticas ("procesador", "inversi√≥n", "vacuna").

### B. K-Means: El Cerebro del Agrupamiento

Para el **Aprendizaje No Supervisado**, el algoritmo de **K-Means** es el est√°ndar de oro para encontrar patrones sin etiquetas previas.

- **El Proceso**:
    1. Define $k$ puntos aleatorios (centroides).
    2. Asigna cada documento al centroide m√°s cercano (usando distancia euclidiana en el espacio vectorial).
    3. Recalcula el centro del grupo y repite hasta que los grupos se estabilizan.
- **Por qu√© lo usamos**: Es extremadamente eficiente para grandes vol√∫menes de datos y nos permite segmentar el mercado, noticias o documentos legales de forma autom√°tica.

### C. PCA: Visualizando el Hiperespacio

Nuestra matriz TF-IDF tiene cientos de dimensiones (una por cada palabra √∫nica). El ser humano solo puede ver en 2D o 3D.

- **¬øQu√© significa?**: **Principal Component Analysis** "comprime" la informaci√≥n. Busca las direcciones (componentes) donde los datos var√≠an m√°s y proyecta todo sobre ellas.
- **Utilidad Did√°ctica**: Sin PCA, el clustering ser√≠a una lista de n√∫meros abstractos. Con PCA, podemos "ver" la separaci√≥n de conceptos en una gr√°fica.

---

## üìä 3. Interpretaci√≥n de lo que estamos viendo

Al ejecutar el c√≥digo, se genera la visualizaci√≥n `05_visualizacion_clustering.png`.

![Clustering de Documentos](05_visualizacion_clustering.png)

### ¬øC√≥mo leer este gr√°fico?

1. **Cercan√≠a es Similitud**: Dos puntos que est√°n pegados significan documentos que comparten palabras clave y, por lo tanto, temas.
2. **Dispersi√≥n de Clusters**:
    - Si los grupos est√°n muy separados, el algoritmo ha tenido √©xito total identificando temas √∫nicos.
    - Si hay solapamiento, indica que hay documentos que comparten vocabulario de varios temas (ej. un art√≠culo sobre "Tecnolog√≠a en la Salud").
3. **Los Ejes (PCA)**: El Eje X (Componente 1) suele capturar la diferencia m√°s grande entre los temas (ej. t√©rminos m√©dicos vs t√©rminos financieros).

---

## üë®‚Äçüè´ 4. Gu√≠a Did√°ctica: Paso a Paso

### Paso 1: Generaci√≥n del Corpus

Creamos 1,200 documentos sint√©ticos. En una formaci√≥n real, esto simula la ingesta de datos de una API o una base de datos SQL.

### Paso 2: Limpieza y Tokenizaci√≥n

Aunque el script es directo, en NLP real eliminar√≠amos puntuaci√≥n, convertir√≠amos a min√∫sculas y quitar√≠amos *Stop Words* (palabras vac√≠as) para que el modelo no se distraiga con ruido.

### Paso 3: Entrenamiento del Modelo

El comando `kmeans.fit(tfidf_matrix)` es donde ocurre la "magia". El modelo "aprende" la estructura latente de los datos sin ayuda humana.

---

## üìö Referencias y Citas Acad√©micas

Para profundizar en la metodolog√≠a, se recomienda la consulta de las siguientes fuentes fundamentales:

- **McKinney, W. (2012).** *Python for Data Analysis*. O'Reilly Media. (Referencia para manipulaci√≥n de matrices).
- **Pedregosa, F., et al. (2011).** *Scikit-learn: Machine Learning in Python*. JMLR. (Documentaci√≥n oficial del framework utilizado).
- **Manning, C. D., et al. (2008).** *Introduction to Information Retrieval*. Cambridge University Press. (Teor√≠a base de TF-IDF).

---

## üéì Informaci√≥n Institucional

**Autor/Referencia:** @TodoEconometria  
**Profesor:** Juan Marcelo Gutierrez Miranda  
**√Årea:** Big Data, Ciencia de Datos & Econometr√≠a Aplicada.  

**Hash ID de Certificaci√≥n:**  
`4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c`
