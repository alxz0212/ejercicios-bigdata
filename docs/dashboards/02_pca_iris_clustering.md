# PCA + Clustering K-Means: Dataset Iris

**Autor:** @TodoEconometria | **Profesor:** Juan Marcelo Gutierrez Miranda

---

## ğŸ“š Tabla de Contenidos

1. [IntroducciÃ³n](#1-introducciÃ³n)
2. [El Dataset Iris: Un ClÃ¡sico del Machine Learning](#2-el-dataset-iris-un-clÃ¡sico-del-machine-learning)
3. [Por QuÃ© Combinar PCA + Clustering](#3-por-quÃ©-combinar-pca--clustering)
4. [AnÃ¡lisis de Componentes Principales (PCA)](#4-anÃ¡lisis-de-componentes-principales-pca)
5. [Clustering K-Means](#5-clustering-k-means)
6. [InterpretaciÃ³n de Resultados](#6-interpretaciÃ³n-de-resultados)
7. [Conclusiones y Recomendaciones](#7-conclusiones-y-recomendaciones)

---

## 1. IntroducciÃ³n

Este documento presenta un **anÃ¡lisis completo** del famoso **dataset Iris** combinando dos tÃ©cnicas fundamentales del Machine Learning no supervisado:

- **PCA (Principal Component Analysis)**: ReducciÃ³n de dimensionalidad
- **K-Means Clustering**: AgrupaciÃ³n de observaciones

### ğŸ¯ Objetivos del AnÃ¡lisis

1. **Reducir** las 4 dimensiones originales a 2 dimensiones principales
2. **Identificar** grupos naturales en los datos (especies de flores)
3. **Visualizar** patrones y relaciones en un espacio 2D
4. **Validar** si el clustering no supervisado puede descubrir las 3 especies conocidas

---

## 2. El Dataset Iris: Un ClÃ¡sico del Machine Learning

### ğŸ“– Historia y Contexto

El dataset Iris fue introducido por **Ronald Fisher** en 1936 en su paper seminal:

> Fisher, R. A. (1936). *The use of multiple measurements in taxonomic problems*. Annals of Eugenics, 7(2), 179-188.

Es uno de los datasets mÃ¡s utilizados en:

- EnseÃ±anza de Machine Learning
- ValidaciÃ³n de algoritmos de clasificaciÃ³n
- Ejemplos de visualizaciÃ³n de datos

### ğŸŒ¸ DescripciÃ³n del Dataset

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **Observaciones** | 150 flores |
| **Especies** | 3 (Setosa, Versicolor, Virginica) |
| **Variables** | 4 medidas en centÃ­metros |
| **DistribuciÃ³n** | 50 flores por especie (balanceado) |

### ğŸ“ Variables Medidas

1. **Sepal Length** (Largo del sÃ©palo)
2. **Sepal Width** (Ancho del sÃ©palo)
3. **Petal Length** (Largo del pÃ©talo)
4. **Petal Width** (Ancho del pÃ©talo)

> **NOTA BOTÃNICA:** El **sÃ©palo** es la parte verde que protege la flor antes de abrirse. El **pÃ©talo** es la parte colorida de la flor.

### ğŸ” Â¿Por QuÃ© es Importante Este Dataset?

1. **TamaÃ±o Manejable**: 150 observaciones son suficientes para aprender sin ser abrumadoras
2. **Bien Balanceado**: 50 flores de cada especie (no hay desbalance de clases)
3. **Separabilidad**: Una especie (Setosa) es linealmente separable, las otras dos se superponen ligeramente
4. **Multivariate**: 4 variables permiten practicar tÃ©cnicas de reducciÃ³n de dimensionalidad

---

## 3. Por QuÃ© Combinar PCA + Clustering

### ğŸ¤” El Problema de la Dimensionalidad

Cuando tenemos **mÃ¡s de 3 dimensiones**, es imposible visualizar los datos directamente:

- **1D**: LÃ­nea (fÃ¡cil)
- **2D**: Plano (fÃ¡cil)
- **3D**: Espacio 3D (posible pero difÃ­cil)
- **4D+**: âŒ **Imposible de visualizar**

### ğŸ’¡ La SoluciÃ³n: PCA + Clustering

```
Datos Originales (4D)
        â†“
    PCA (ReducciÃ³n)
        â†“
Datos Reducidos (2D) â† Ahora podemos VISUALIZAR
        â†“
    K-Means (AgrupaciÃ³n)
        â†“
  Clusters Identificados
```

### âœ… Ventajas de Esta CombinaciÃ³n

| Ventaja | ExplicaciÃ³n |
|---------|-------------|
| **VisualizaciÃ³n** | PCA reduce a 2D para graficar |
| **ReducciÃ³n de Ruido** | PCA elimina varianza no informativa |
| **Mejor Clustering** | K-Means funciona mejor en espacios de menor dimensiÃ³n |
| **Interpretabilidad** | Podemos ver y entender los clusters en 2D |

---

## 4. AnÃ¡lisis de Componentes Principales (PCA)

### ğŸ¯ Â¿QuÃ© es PCA?

**PCA** es una tÃ©cnica que:

1. **Encuentra** las direcciones de mÃ¡xima varianza en los datos
2. **Proyecta** los datos en esas direcciones (componentes principales)
3. **Reduce** la dimensionalidad manteniendo la mayor informaciÃ³n posible

### ğŸ“Š Resultados del PCA en Iris

#### Varianza Explicada

| DimensiÃ³n | Autovalor | Varianza (%) | Varianza Acumulada (%) |
|-----------|-----------|--------------|------------------------|
| **Dim.1** | ~2.92 | ~73% | ~73% |
| **Dim.2** | ~0.91 | ~23% | ~96% |
| Dim.3 | ~0.15 | ~4% | ~99% |
| Dim.4 | ~0.02 | ~1% | ~100% |

> **INTERPRETACIÃ“N:** Las primeras 2 dimensiones capturan **~96%** de la varianza total. Esto significa que podemos reducir de 4D a 2D perdiendo solo ~4% de informaciÃ³n.

#### Regla de Kaiser

La **Regla de Kaiser** dice: *Retener componentes con autovalor > 1*

- **Dim.1**: Autovalor = 2.92 âœ… (Retener)
- **Dim.2**: Autovalor = 0.91 âš ï¸ (Casi 1, retener para visualizaciÃ³n)
- **Dim.3**: Autovalor = 0.15 âŒ (Descartar)
- **Dim.4**: Autovalor = 0.02 âŒ (Descartar)

### ğŸ” InterpretaciÃ³n de las Dimensiones

#### DimensiÃ³n 1 (~73% de varianza)

**Variables que mÃ¡s contribuyen:**

- **Petal Length** (~42%)
- **Petal Width** (~42%)

**InterpretaciÃ³n:**
> Dim.1 representa el **"tamaÃ±o del pÃ©talo"**. Flores con valores altos en Dim.1 tienen pÃ©talos grandes; valores bajos tienen pÃ©talos pequeÃ±os.

#### DimensiÃ³n 2 (~23% de varianza)

**Variables que mÃ¡s contribuyen:**

- **Sepal Width** (~72%)

**InterpretaciÃ³n:**
> Dim.2 representa el **"ancho del sÃ©palo"**. Flores con valores altos en Dim.2 tienen sÃ©palos anchos; valores bajos tienen sÃ©palos estrechos.

### ğŸ“ˆ CÃ­rculo de CorrelaciÃ³n

El **cÃ­rculo de correlaciÃ³n** muestra cÃ³mo las variables originales se relacionan con las dimensiones principales:

```
           Dim.2 (Sepal Width)
                 â†‘
                 |
    Sepal Width  |
         â†‘       |
         |       |
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Dim.1 (Petal Size)
         |       |
         |   Petal Length â†’
         |   Petal Width â†’
         |
```

**Observaciones:**

- **Petal Length** y **Petal Width** estÃ¡n muy correlacionadas (flechas en la misma direcciÃ³n)
- **Sepal Width** es casi perpendicular a las medidas de pÃ©talo (baja correlaciÃ³n)
- **Sepal Length** estÃ¡ entre ambas dimensiones

---

## 5. Clustering K-Means

### ğŸ¯ Â¿QuÃ© es K-Means?

**K-Means** es un algoritmo de clustering que:

1. **Divide** los datos en K grupos (clusters)
2. **Minimiza** la distancia de cada punto a su centroide
3. **Itera** hasta convergencia

### ğŸ”¢ DeterminaciÃ³n del NÃºmero Ã“ptimo de Clusters

#### MÃ©todo del Codo (Elbow Method)

Graficamos la **inercia** (suma de distancias al cuadrado) vs K:

```
Inercia
  â”‚
  â”‚ â—
  â”‚   â—
  â”‚     â—  â† "Codo" en K=3
  â”‚       â—
  â”‚         â—
  â”‚           â—
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ K
    2  3  4  5  6  7
```

**InterpretaciÃ³n:** El "codo" estÃ¡ en **K=3**, sugiriendo 3 clusters.

#### Silhouette Score

El **Silhouette Score** mide quÃ© tan bien separados estÃ¡n los clusters:

- **Valor**: Entre -1 y 1
- **InterpretaciÃ³n**:
  - Cercano a **1**: Clusters bien separados âœ…
  - Cercano a **0**: Clusters superpuestos âš ï¸
  - Negativo: Puntos mal asignados âŒ

**Resultado para Iris:** Silhouette Score â‰ˆ **0.55** (buena separaciÃ³n)

### ğŸ“Š Resultados del Clustering

#### Confusion Matrix: Clusters vs Especies Reales

|           | Cluster 0 | Cluster 1 | Cluster 2 |
|-----------|-----------|-----------|-----------|
| **Setosa**     | 50 | 0 | 0 |
| **Versicolor** | 0 | 48 | 2 |
| **Virginica**  | 0 | 14 | 36 |

**Observaciones:**

- **Setosa**: Perfectamente separada (100% en Cluster 0)
- **Versicolor**: Mayormente en Cluster 1 (96%)
- **Virginica**: Mayormente en Cluster 2 (72%), pero con superposiciÃ³n con Versicolor

#### Pureza de Clusters

La **pureza** mide el porcentaje de observaciones correctamente agrupadas:

```
Pureza = (50 + 48 + 36) / 150 = 89.3%
```

> **INTERPRETACIÃ“N:** El algoritmo K-Means logrÃ³ identificar correctamente las especies en **89.3%** de los casos, **sin conocer las etiquetas reales**. Esto es excelente para un mÃ©todo no supervisado.

### ğŸ¨ VisualizaciÃ³n de Clusters

En el espacio 2D del PCA, los clusters se ven asÃ­:

```
     Dim.2
       â†‘
       â”‚     â— Cluster 2 (Virginica)
       â”‚    â—â—â—
       â”‚   â—â—â—â—
       â”‚  â—â—â—â—
       â”‚ â—â—â—â—  â– â– â–  Cluster 1 (Versicolor)
       â”‚â—â—â—   â– â– â– â– 
â”€â”€â”€â”€â”€â”€â”€â”¼â– â– â– â– â– â– â– â– â– â”€â”€â”€â”€â”€â”€â†’ Dim.1
       â”‚
       â”‚  â–²â–²â–²
       â”‚ â–²â–²â–²â–²â–²
       â”‚â–²â–²â–²â–²â–²â–²  Cluster 0 (Setosa)
       â”‚
```

**Centroides** (marcados con X):

- **Cluster 0**: (-2.7, 0.3) â†’ Setosa
- **Cluster 1**: (0.3, -0.5) â†’ Versicolor
- **Cluster 2**: (1.7, 0.2) â†’ Virginica

---

## 6. InterpretaciÃ³n de Resultados

### ğŸ”¬ AnÃ¡lisis por Especie

#### Setosa (Cluster 0)

**CaracterÃ­sticas:**

- **Petal Length**: Muy pequeÃ±o (~1.5 cm)
- **Petal Width**: Muy pequeÃ±o (~0.2 cm)
- **Sepal Width**: Relativamente grande

**PosiciÃ³n en PCA:**

- **Dim.1**: Valores muy negativos (pÃ©talos pequeÃ±os)
- **Dim.2**: Valores positivos (sÃ©palos anchos)

**Separabilidad:** âœ… **Perfecta** (100% correctamente agrupada)

#### Versicolor (Cluster 1)

**CaracterÃ­sticas:**

- **Petal Length**: Mediano (~4.3 cm)
- **Petal Width**: Mediano (~1.3 cm)
- **Sepal Width**: Mediano

**PosiciÃ³n en PCA:**

- **Dim.1**: Valores cercanos a 0 (pÃ©talos medianos)
- **Dim.2**: Valores ligeramente negativos

**Separabilidad:** âš ï¸ **Buena** (96% correctamente agrupada, 4% confundida con Virginica)

#### Virginica (Cluster 2)

**CaracterÃ­sticas:**

- **Petal Length**: Grande (~5.5 cm)
- **Petal Width**: Grande (~2.0 cm)
- **Sepal Width**: Mediano

**PosiciÃ³n en PCA:**

- **Dim.1**: Valores muy positivos (pÃ©talos grandes)
- **Dim.2**: Valores cercanos a 0

**Separabilidad:** âš ï¸ **Moderada** (72% correctamente agrupada, 28% confundida con Versicolor)

### ğŸ“Š MÃ©tricas de EvaluaciÃ³n

| MÃ©trica | Valor | InterpretaciÃ³n |
|---------|-------|----------------|
| **Silhouette Score** | 0.55 | Buena separaciÃ³n entre clusters |
| **Davies-Bouldin Index** | 0.66 | Clusters compactos y separados (menor es mejor) |
| **Calinski-Harabasz Index** | 561.63 | Alta separaciÃ³n entre clusters (mayor es mejor) |
| **Pureza** | 89.3% | Alta concordancia con especies reales |

### ğŸ¯ Â¿Por QuÃ© Versicolor y Virginica se Superponen?

**RazÃ³n BiolÃ³gica:**

- Versicolor y Virginica son especies **evolutivamente mÃ¡s cercanas**
- Comparten caracterÃ­sticas morfolÃ³gicas similares
- Setosa es mÃ¡s distinta (probablemente de un linaje diferente)

**RazÃ³n EstadÃ­stica:**

- Las medidas de pÃ©talo de Versicolor y Virginica tienen **rangos superpuestos**
- No existe una frontera clara en el espacio de 4 dimensiones

---

## 7. Conclusiones y Recomendaciones

### âœ… Conclusiones Principales

1. **PCA es Efectivo**:
   - Reduce de 4D a 2D manteniendo **96%** de la informaciÃ³n
   - Las 2 primeras dimensiones son suficientes para visualizaciÃ³n y clustering

2. **Las Medidas de PÃ©talo son Clave**:
   - **Petal Length** y **Petal Width** son las variables mÃ¡s discriminantes
   - Dim.1 (que representa el tamaÃ±o del pÃ©talo) explica **73%** de la varianza

3. **K-Means Funciona Bien**:
   - Identifica correctamente las 3 especies en **89.3%** de los casos
   - Setosa es perfectamente separable
   - Versicolor y Virginica tienen cierta superposiciÃ³n natural

4. **ValidaciÃ³n del MÃ©todo No Supervisado**:
   - Sin conocer las etiquetas, K-Means descubre los 3 grupos naturales
   - Esto valida que las especies tienen diferencias morfolÃ³gicas reales

### ğŸ“ Lecciones para Estudiantes

#### LecciÃ³n 1: La Importancia de la ReducciÃ³n de Dimensionalidad

> **ANTES DE PCA:** 4 variables â†’ DifÃ­cil de visualizar â†’ DifÃ­cil de interpretar
>
> **DESPUÃ‰S DE PCA:** 2 dimensiones â†’ FÃ¡cil de visualizar â†’ Patrones claros

**Moraleja:** No siempre necesitas todas las variables. A veces, menos es mÃ¡s.

#### LecciÃ³n 2: El Clustering No Supervisado Puede Descubrir Estructura Real

> **SIN ETIQUETAS:** K-Means encuentra 3 grupos
>
> **CON ETIQUETAS:** Hay 3 especies reales
>
> **COINCIDENCIA:** 89.3%

**Moraleja:** Los datos tienen estructura natural. Los algoritmos pueden encontrarla.

#### LecciÃ³n 3: No Todos los Grupos son Perfectamente Separables

> **Setosa:** 100% separable
>
> **Versicolor/Virginica:** SuperposiciÃ³n natural

**Moraleja:** En datos reales, la superposiciÃ³n es normal. No esperes clusters perfectos.

#### LecciÃ³n 4: Validar, Validar, Validar

> **MÃ©todo del Codo:** Sugiere K=3
>
> **Silhouette Score:** Confirma K=3
>
> **Pureza:** Valida que K=3 es correcto

**Moraleja:** Usa mÃºltiples mÃ©tricas para validar tus decisiones.

### ğŸ”§ Recomendaciones PrÃ¡cticas

#### Para ClasificaciÃ³n de Especies de Iris

1. **Enfocarse en medidas de pÃ©talo** (son las mÃ¡s discriminantes)
2. **Usar PCA para visualizaciÃ³n** (reduce complejidad sin perder informaciÃ³n)
3. **K=3 es Ã³ptimo** (validado por mÃºltiples mÃ©tricas)

#### Para AnÃ¡lisis de Datos Similares

1. **Siempre hacer EDA primero** (entender distribuciones y correlaciones)
2. **Estandarizar antes de PCA** (variables en diferentes escalas sesgan resultados)
3. **Validar nÃºmero de clusters** (no asumir K, usar Elbow + Silhouette)
4. **Comparar con ground truth** (si estÃ¡ disponible, como en este caso)

### ğŸš€ Extensiones Posibles

1. **Otros Algoritmos de Clustering**:
   - DBSCAN (para clusters de forma arbitraria)
   - Hierarchical Clustering (para dendrogramas)
   - Gaussian Mixture Models (para clusters probabilÃ­sticos)

2. **ClasificaciÃ³n Supervisada**:
   - Usar las especies conocidas para entrenar un clasificador
   - Comparar con clustering no supervisado

3. **AnÃ¡lisis de Variables Suplementarias**:
   - Agregar informaciÃ³n de ubicaciÃ³n geogrÃ¡fica
   - Agregar informaciÃ³n de temporada de recolecciÃ³n

---

## ğŸ“š Referencias

### Papers Originales

- **Fisher, R. A. (1936).** *The use of multiple measurements in taxonomic problems*. Annals of Eugenics, 7(2), 179-188.
  - El paper original que introdujo el dataset Iris

- **Anderson, E. (1935).** *The irises of the Gaspe Peninsula*. Bulletin of the American Iris Society, 59, 2-5.
  - El botÃ¡nico que recolectÃ³ los datos originales

### Libros de Referencia

- **Husson, F., LÃª, S., & PagÃ¨s, J. (2017).** *Exploratory Multivariate Analysis by Example Using R*. CRC Press.
  - Referencia principal para PCA estilo FactoMineR

- **James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013).** *An Introduction to Statistical Learning*. Springer.
  - CapÃ­tulos sobre PCA y Clustering

### ArtÃ­culos TÃ©cnicos

- **Pedregosa, F., et al. (2011).** *Scikit-learn: Machine Learning in Python*. JMLR 12, pp. 2825-2830.
  - DocumentaciÃ³n de las librerÃ­as utilizadas

- **Rousseeuw, P. J. (1987).** *Silhouettes: A graphical aid to the interpretation and validation of cluster analysis*. Journal of Computational and Applied Mathematics, 20, 53-65.
  - MÃ©todo del Silhouette Score

---

## ğŸ”— Recursos Adicionales

### Tutoriales Online

- [Scikit-learn: PCA Tutorial](https://scikit-learn.org/stable/modules/decomposition.html#pca)
- [Scikit-learn: K-Means Tutorial](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [FactoMineR Tutorial](http://factominer.free.fr/course/FactoTuto.html)

### Datasets Similares

- **Wine Dataset**: 178 vinos, 13 variables quÃ­micas, 3 clases
- **Breast Cancer Dataset**: 569 tumores, 30 variables, 2 clases (maligno/benigno)
- **Digits Dataset**: 1797 imÃ¡genes de dÃ­gitos, 64 pÃ­xeles, 10 clases

---

**Autor:** @TodoEconometria  
**Profesor:** Juan Marcelo Gutierrez Miranda  
**Fecha:** Enero 2026  
**Licencia:** Uso educativo con atribuciÃ³n

---

## ğŸ’¬ Preguntas Frecuentes (FAQ)

### Â¿Por quÃ© estandarizar antes de PCA?

**Respuesta:** Porque PCA es sensible a la escala de las variables. Si una variable tiene valores mucho mayores que otra (ej: ingresos en miles vs edad en decenas), dominarÃ¡ la varianza y sesgarÃ¡ los resultados.

### Â¿CuÃ¡ntas componentes debo retener?

**Respuesta:** Depende del objetivo:

- **VisualizaciÃ³n**: 2-3 componentes
- **Regla de Kaiser**: Componentes con autovalor > 1
- **Varianza Acumulada**: Retener hasta alcanzar 80-95% de varianza

### Â¿K-Means siempre encuentra los clusters correctos?

**Respuesta:** No. K-Means tiene limitaciones:

- Asume clusters esfÃ©ricos
- Sensible a inicializaciÃ³n (usar `n_init` alto)
- Requiere especificar K de antemano

### Â¿QuÃ© pasa si tengo mÃ¡s de 3 especies?

**Respuesta:** El proceso es el mismo:

1. Usar Elbow + Silhouette para determinar K Ã³ptimo
2. Validar con mÃ©tricas (pureza, confusion matrix)
3. Visualizar en 2D con PCA (aunque haya mÃ¡s de 3 clusters)

---

**FIN DEL DOCUMENTO**
