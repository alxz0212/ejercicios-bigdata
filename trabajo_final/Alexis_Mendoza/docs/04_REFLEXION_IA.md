# Paso 4: Reflexi√≥n IA - "3 Momentos Clave"

**Alumno:** Daniel Alexis Mendoza Corne  
**Fecha:** Febrero 2026

---

## Bloque A: Infraestructura (Docker)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
Le ped√≠ ayuda para crear el archivo `docker-compose.yml`. No ten√≠a muy claro c√≥mo conectar Spark (el Master y el Worker) con JupyterLab y la base de datos Postgres, as√≠ que le ped√≠ que me generara la estructura b√°sica para que todo funcionara junto.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Al principio no pod√≠a entrar a los servicios. Intentaba poner en el navegador los puertos que ve√≠a en el archivo, como el `7077` o el `5432`, y me sal√≠a error de p√°gina no encontrada.

- **Resoluci√≥n:** La IA me explic√≥ que esos puertos son internos para que se hablen las m√°quinas entre ellas. Para yo ver algo, ten√≠a que usar los puertos "web" o visuales, que eran el `8080` para ver Spark y el `8888` para mi Jupyter. ¬°Vaya l√≠o de puertos!

**Otro problema: PySpark no aparec√≠a**

- **Fallo:** Cuando corr√≠a mi c√≥digo, me dec√≠a `ModuleNotFoundError: No module named 'pyspark'`.
- **Soluci√≥n:** Resulta que aunque la imagen de Docker ten√≠a Spark, mi script de Python no lo encontraba. Tuve que a√±adir `pyspark==3.5.0` al archivo `requirements.txt` y volver a construir la imagen.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
Entend√≠ la diferencia entre los puertos que "expongo" hacia afuera (para m√≠) y los que se quedan dentro de la red de Docker. Tambi√©n aprend√≠ a usar `volumes` para guardar mis notebooks, porque la primera vez reinici√© el contenedor y... ¬°adi√≥s trabajo!

**Otro Error Detectado: Spark Worker Offline**

- **Fallo:** En la interfaz `localhost:8080`, aparec√≠a "Alive Workers: 0" aunque el contenedor exist√≠a.
- **Causa:** Al reconstruir y levantar solo el servicio `jupyter-lab`, docker-compose no necesariamente reinicia o mantiene activos los contenedores dependientes si no se especifican.
- **Resoluci√≥n:** Ejecutar `docker-compose up -d` (sin especificar servicio) y verificar con `docker ps` asegur√≥ que tanto Master como Worker estuvieran activos.
- **Aprendizaje:** La "Arquitectura Distribuida" requiere validaci√≥n expl√≠cita de que todos los nodos est√°n vivos, no basta con que el c√≥digo corra (que puede estar en modo local).

### üí¨ Prompt Clave (Bloque A)

```text
"Genera la configuraci√≥n de docker-compose.yml para un entorno Spark clusterizado (1 Master, 1 Worker) con persistencia de datos en PostgreSQL. Aseg√∫rate de exponer los puertos UI (8080, 4040) y configurar la red bridge para que JupyterLab pueda acceder al Spark Master mediante el nombre del servicio."
```

---

## Bloque B: Pipeline ETL (Spark)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
Necesitaba ayuda para hacer el script `pipeline.py`. Quer√≠a que leyera el dataset QoG pero que solo se quedara con los 5 pa√≠ses que me interesaban del "Gran Juego" y que adem√°s me creara una columna nueva para agruparlos por zona.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Tuve problemas graves al intentar subir todo a GitHub.
**Error:** `fatal: not a git repository`.

- **Resoluci√≥n:** Me hab√≠a olvidado de iniciar el repositorio con `git init`. La IA me guio paso a paso: iniciar git, configurar el `.gitignore` (s√∫per importante para no subir datos pesados por error) y luego vincularlo con mi repo en GitHub.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
Aprend√≠ a usar `pyspark.sql.functions.when`. Antes hac√≠a esto con bucles `for` en Python normal, pero con Big Data eso es lent√≠simo. Con esta funci√≥n de Spark, puedo crear columnas condicionales (como la de `subregion`) de forma s√∫per r√°pida y distribuida.

### üí¨ Prompt Clave (Bloque B)

```text
"Quiero subir mi proyecto a GitHub pero evitar errores. Dame los pasos exactos para iniciar el repositorio, crear un archivo `.gitignore` que excluya mis datos pesados (carpeta /data) y los archivos temporales de Jupyter, y finalmente c√≥mo conectar mi carpeta local con el repositorio remoto main."
```

---

## Bloque C: An√°lisis de Datos (Machine Learning)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
Le pregunt√© qu√© modelo de Machine Learning me conven√≠a m√°s. Estaba dudando entre KNN, SVM o Random Forest para ver c√≥mo influ√≠an las variables pol√≠ticas en la econom√≠a.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Quise generar las gr√°ficas autom√°ticamente corriendo el notebook desde la terminal, pero todo explot√≥.
**Error:** `TypeError: 'JavaPackage' object is not callable`.

- **Resoluci√≥n:** La IA me sugiri√≥ que no mezclara cosas. En lugar de forzar el notebook, pas√© la l√≥gica a un script limpio en Python (`src/analysis.py`) y lo ejecut√© con `spark-submit`. Funcion√≥ mucho mejor y sin conflictos raros de Java.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
Que **Random Forest** es genial no solo para predecir, sino para explicar **por qu√©** predice lo que predice (feature importance). Eso me ayud√≥ mucho m√°s que KNN para entender mi problema de investigaci√≥n. Tambi√©n aprend√≠ que automatizar gr√°ficas es mejor que hacerlas a mano una por una.

### üí¨ Prompt Clave (Bloque C)

```text
"Act√∫a como un experto en Data Science. Tengo un dataset con variables sociopol√≠ticas y quiero predecir el impacto en el PIB. Eval√∫a comparativamente KNN, SVM y Random Forest justificando cu√°l es m√°s adecuado considerando la explicabilidad (feature importance), manejo de outliers y la dimensionalidad de mis datos."
```
