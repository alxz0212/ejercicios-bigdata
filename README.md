# Big Data con Python - De Cero a Producci√≥n

<div align="center">

<img src="docs/assets/todoeconometria_logo.png" alt="TodoEconometria" width="350">

*"Sin experiencia no hay conocimiento"*

## CURSO COMPLETO DE BIG DATA

[![GitHub Stars](https://img.shields.io/github/stars/TodoEconometria/ejercicios-bigdata?style=for-the-badge&logo=github)](https://github.com/TodoEconometria/ejercicios-bigdata/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/TodoEconometria/ejercicios-bigdata?style=for-the-badge&logo=github)](https://github.com/TodoEconometria/ejercicios-bigdata/network/members)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![Deploy](https://github.com/TodoEconometria/ejercicios-bigdata/actions/workflows/docs.yml/badge.svg)](https://github.com/TodoEconometria/ejercicios-bigdata/actions/workflows/docs.yml)

### [**Ver Sitio Web del Curso**](https://todoeconometria.github.io/ejercicios-bigdata/)

<<<<<<< HEAD
### Paso 1: Haz un Fork de este Repositorio

Un **fork** es tu copia personal de este proyecto donde trabajar√°s sin afectar el original.

1. Haz clic en el bot√≥n **Fork** (arriba a la derecha en GitHub)
2. Selecciona tu cuenta personal
3. ¬°Listo! Ahora tienes tu propia copia

### Paso 2: Configura tu Proyecto

**IMPORTANTE**: Sigue las instrucciones completas en:
### üëâ **[INSTRUCCIONES_CONFIGURACION.md](./INSTRUCCIONES_CONFIGURACION.md)** üëà

Este archivo te guiar√° paso a paso para:
- Clonar el repositorio
- Crear las carpetas de trabajo
- Copiar las plantillas de ejercicios
- Instalar Python y dependencias
- Descargar los datos

### Paso 3: Lee las Gu√≠as

Este repositorio incluye gu√≠as para ayudarte:

- **[INSTRUCCIONES_CONFIGURACION.md](./INSTRUCCIONES_CONFIGURACION.md)**: Configuraci√≥n inicial (¬°empieza aqu√≠!)
- **[GUIA_GIT_GITHUB.md](./GUIA_GIT_GITHUB.md)**: Todo sobre Git y GitHub
- **[GUIA_IA_ASISTENTE.md](./GUIA_IA_ASISTENTE.md)**: C√≥mo usar IA para aprender (Gemini, Claude, ChatGPT)
- **[LEEME.md](./LEEME.md)**: Instrucciones t√©cnicas de los ejercicios
- **[ARQUITECTURA_Y_STACK.md](./ARQUITECTURA_Y_STACK.md)**: Conceptos avanzados (opcional)

### Paso 4: Prepara tu Entorno

Necesitar√°s:
- **Python 3.8+** instalado ([Descargar aqu√≠](https://www.python.org/downloads/))
- **Git** instalado ([Descargar aqu√≠](https://git-scm.com/downloads))
- Un editor de c√≥digo (recomendamos [VS Code](https://code.visualstudio.com/) o PyCharm)

## Estructura del Proyecto

```
ejercicios_bigdata/
‚îú‚îÄ‚îÄ README.md                       # Este archivo
‚îú‚îÄ‚îÄ INSTRUCCIONES_CONFIGURACION.md  # Configuraci√≥n inicial (¬°LEE ESTO PRIMERO!)
‚îú‚îÄ‚îÄ GUIA_GIT_GITHUB.md              # Gu√≠a de Git para principiantes
‚îú‚îÄ‚îÄ GUIA_IA_ASISTENTE.md            # Gu√≠a para usar IA (Gemini, Claude, ChatGPT)
‚îú‚îÄ‚îÄ LEEME.md                        # Instrucciones t√©cnicas de ejercicios
‚îú‚îÄ‚îÄ ARQUITECTURA_Y_STACK.md         # Conceptos t√©cnicos
‚îú‚îÄ‚îÄ requirements.txt                # Librer√≠as Python necesarias
‚îú‚îÄ‚îÄ PROGRESO.md                     # Tu checklist de avance
‚îú‚îÄ‚îÄ plantillas/                     # Plantillas originales (NO modificar)
‚îÇ   ‚îú‚îÄ‚îÄ datos/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ descargar_datos.py      # Plantilla del script de descarga
‚îÇ   ‚îî‚îÄ‚îÄ ejercicios/
‚îÇ       ‚îú‚îÄ‚îÄ 01_cargar_sqlite.py     # Plantilla ejercicio 1
‚îÇ       ‚îú‚îÄ‚îÄ 02_limpieza_datos.py    # Plantilla ejercicio 2
‚îÇ       ‚îú‚îÄ‚îÄ 03_parquet_dask.py      # Plantilla ejercicio 3
‚îÇ       ‚îî‚îÄ‚îÄ 04_pyspark_query.py     # Plantilla ejercicio 4
‚îú‚îÄ‚îÄ datos/                          # TU carpeta (crear√°s despu√©s)
‚îÇ   ‚îî‚îÄ‚îÄ descargar_datos.py          # Tu copia para trabajar
‚îî‚îÄ‚îÄ ejercicios/                     # TU carpeta (crear√°s despu√©s)
    ‚îú‚îÄ‚îÄ 01_cargar_sqlite.py         # Tu ejercicio 1
    ‚îú‚îÄ‚îÄ 02_limpieza_datos.py        # Tu ejercicio 2
    ‚îú‚îÄ‚îÄ 03_parquet_dask.py          # Tu ejercicio 3
    ‚îî‚îÄ‚îÄ 04_pyspark_query.py         # Tu ejercicio 4
```

**Nota**: Las carpetas `datos/` y `ejercicios/` NO est√°n en el repositorio inicial. Las crear√°s siguiendo **[INSTRUCCIONES_CONFIGURACION.md](./INSTRUCCIONES_CONFIGURACION.md)**.

## C√≥mo Trabajar en este Proyecto

### Sigue este Orden:

1. **Lee** ‚Üí [INSTRUCCIONES_CONFIGURACION.md](./INSTRUCCIONES_CONFIGURACION.md) (configuraci√≥n completa)
2. **Lee** ‚Üí [GUIA_GIT_GITHUB.md](./GUIA_GIT_GITHUB.md) (si no conoces Git)
3. **Lee** ‚Üí [GUIA_IA_ASISTENTE.md](./GUIA_IA_ASISTENTE.md) (c√≥mo usar Gemini y otras IAs)
4. **Lee** ‚Üí [LEEME.md](./LEEME.md) (instrucciones de ejercicios)
5. **Comienza** ‚Üí Ejercicio 01_cargar_sqlite.py
6. **Actualiza** ‚Üí PROGRESO.md despu√©s de cada ejercicio
7. **Haz commit y push** ‚Üí Sube tu progreso a GitHub regularmente

## Seguimiento de tu Progreso

1. Abre el archivo [PROGRESO.md](./PROGRESO.md)
2. Marca ‚úÖ cada ejercicio que completes
3. Haz commit de tus cambios regularmente
4. Sube (push) tus commits a GitHub

Tu profesor podr√° ver tu progreso en tu fork.

## C√≥mo Pedir Ayuda

### Opci√≥n 1: Usa IA como Asistente
Lee la [GUIA_IA_ASISTENTE.md](./GUIA_IA_ASISTENTE.md) para aprender a pedir ayuda a herramientas como:
- Claude Code
- GitHub Copilot
- ChatGPT

### Opci√≥n 2: Abre un Issue
Si tienes problemas:
1. Ve a la pesta√±a "Issues" en tu fork
2. Crea un nuevo issue describiendo el problema
3. Comparte el enlace con tu profesor

### Opci√≥n 3: Pregunta a tu Profesor
Comparte el enlace de tu fork con tu profesor para que vea tu c√≥digo.

## Reglas de Oro

1. **No tengas miedo de equivocarte**: Los errores son parte del aprendizaje
2. **Haz commits frecuentes**: Guarda tu progreso regularmente
3. **Lee los comentarios del c√≥digo**: Ah√≠ est√° la explicaci√≥n
4. **Usa la IA con criterio**: √ösala para entender, no solo para copiar
5. **Pide ayuda cuando la necesites**: Nadie nace sabiendo

## Recursos Adicionales

- [Documentaci√≥n oficial de Python](https://docs.python.org/es/)
- [Documentaci√≥n de Pandas](https://pandas.pydata.org/docs/)
- [Tutorial interactivo de Git](https://learngitbranching.js.org/?locale=es_ES)
- [Curso gratuito de Big Data (YouTube)](https://www.youtube.com/results?search_query=big+data+python+tutorial+espa√±ol)

## Licencia

Este material es de uso educativo. Si√©ntete libre de aprender y compartir.
=======
</div>
>>>>>>> upstream/main

---

## Demos en Vivo

| Observatorio S√≠smico Global | ISS Tracker |
|:---------------------------:|:-----------:|
| Sismos en tiempo real desde USGS API | Rastrea la Estaci√≥n Espacial Internacional |
| [**Ver Demo**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_sismos_global.html) | [**Ver Demo**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_iss_tracker.html) |

*Dashboards con datos reales de APIs p√∫blicas, actualizados autom√°ticamente*

---

## El Curso en N√∫meros

| 230 Horas | 9 M√≥dulos | 25+ Ejercicios | 12+ Dashboards | 30+ Tecnolog√≠as |
|:---------:|:---------:|:--------------:|:--------------:|:---------------:|
| de contenido | completos | pr√°cticos | interactivos | profesionales |

---

## Stack Tecnol√≥gico Completo

### Bases de Datos

| Tecnolog√≠a | Nivel | Qu√© Aprender√°s |
|------------|-------|----------------|
| **SQLite** | B√°sico | Queries SQL, √≠ndices, optimizaci√≥n |
| **PostgreSQL** | Intermedio | Joins complejos, Window Functions, CTEs |
| **Oracle** | Avanzado | PL/SQL, procedimientos almacenados |
| **DynamoDB** | Avanzado | NoSQL, key-value, serverless |

### Procesamiento de Datos

| Tecnolog√≠a | Cu√°ndo Usarla | Escala |
|------------|---------------|--------|
| **Pandas** | An√°lisis exploratorio | < 5 GB |
| **Dask** | Datasets grandes, 1 m√°quina | 5-100 GB |
| **Apache Spark** | Clusters, producci√≥n | > 100 GB |
| **Spark Streaming** | Datos en tiempo real | Ilimitado |

### Streaming y Cloud

| Tecnolog√≠a | Prop√≥sito |
|------------|-----------|
| **Apache Kafka** | Streaming distribuido (KRaft mode) |
| **Spark Structured Streaming** | Procesamiento de streams |
| **LocalStack** | Simulaci√≥n AWS local (gratis) |
| **Terraform** | Infraestructura como C√≥digo |
| **AWS S3/Lambda** | Almacenamiento y funciones serverless |

### Machine Learning e IA

| Tecnolog√≠a | Aplicaci√≥n |
|------------|------------|
| **Scikit-learn** | ML cl√°sico, clustering, clasificaci√≥n |
| **TensorFlow** | Deep Learning, redes neuronales |
| **MobileNetV2** | Transfer Learning, Computer Vision |
| **ARIMA/SARIMA** | Series temporales, forecasting |

### NLP y Visualizaci√≥n

| Tecnolog√≠a | Uso |
|------------|-----|
| **NLTK** | Procesamiento de lenguaje natural |
| **TF-IDF** | Vectorizaci√≥n de texto |
| **Plotly** | Dashboards interactivos |
| **Leaflet.js** | Mapas interactivos |

### Econometr√≠a

| Tecnolog√≠a | Aplicaci√≥n |
|------------|------------|
| **linearmodels** | Datos de panel |
| **Panel OLS** | Efectos fijos y aleatorios |
| **Hausman Test** | Selecci√≥n de modelo |

---

## M√≥dulos del Curso

### M√≥dulo 1: Bases de Datos
> SQLite, PostgreSQL, Oracle, migraciones

Desde tu primera query SELECT hasta procedimientos almacenados en Oracle.

### M√≥dulo 2: Limpieza de Datos y ETL
> Pipeline ETL profesional, QoG Dataset (1289 variables, 194+ pa√≠ses)

Pipelines profesionales que procesan millones de registros.

### M√≥dulo 3: Procesamiento Distribuido
> Dask, Parquet, LocalCluster

Procesamiento paralelo de datasets grandes en una sola m√°quina.

### M√≥dulo 4: Machine Learning
> PCA, K-Means, Transfer Learning, ARIMA/SARIMA

Desde clustering hasta Computer Vision con TensorFlow y series temporales.

### M√≥dulo 5: NLP y Text Mining
> NLTK, TF-IDF, Jaccard, An√°lisis de Sentimiento

Tokenizaci√≥n, limpieza, similitud de documentos y an√°lisis de sentimiento.

### M√≥dulo 6: An√°lisis de Datos de Panel
> Efectos Fijos, Efectos Aleatorios, Hausman Test

An√°lisis longitudinal con datos pa√≠s x a√±o.

### M√≥dulo 7: Infraestructura Big Data
> Docker, Docker Compose, Apache Spark Cluster

Contenedores, orquestaci√≥n y clusters Spark en Docker.

### M√≥dulo 8: Streaming con Kafka
> Apache Kafka (KRaft), Spark Structured Streaming

Streaming en tiempo real con datos de sismos desde USGS API.

### M√≥dulo 9: Cloud con LocalStack
> LocalStack, Terraform, AWS S3/Lambda/DynamoDB

Simulaci√≥n de AWS sin costos e Infraestructura como C√≥digo.

### Trabajo Final
> Docker + Spark + PostgreSQL + An√°lisis Completo

Proyecto integrador de principio a fin.

---

## Inicio R√°pido

```bash
# 1. Clona tu fork
git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git
cd ejercicios-bigdata

# 2. Crea entorno virtual
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# 3. Instala dependencias
pip install -r requirements.txt
```

**Siguiente paso:** [Ver documentaci√≥n completa](https://todoeconometria.github.io/ejercicios-bigdata/)

---

## Estructura del Repositorio

```
ejercicios-bigdata/
‚îú‚îÄ‚îÄ ejercicios/                 # C√≥digo por m√≥dulo
‚îÇ   ‚îú‚îÄ‚îÄ 01_bases_de_datos/
‚îÇ   ‚îú‚îÄ‚îÄ 02_limpieza_datos/
‚îÇ   ‚îú‚îÄ‚îÄ 03_procesamiento_distribuido/
‚îÇ   ‚îú‚îÄ‚îÄ 04_machine_learning/
‚îÇ   ‚îú‚îÄ‚îÄ 05_nlp_text_mining/
‚îÇ   ‚îú‚îÄ‚îÄ 06_analisis_datos_de_panel/
‚îÇ   ‚îú‚îÄ‚îÄ 07_infraestructura_bigdata/
‚îÇ   ‚îú‚îÄ‚îÄ 08_streaming_kafka/
‚îÇ   ‚îî‚îÄ‚îÄ 09_cloud_localstack/
‚îÇ
‚îú‚îÄ‚îÄ entregas/                   # Zona de entregas del alumno
‚îú‚îÄ‚îÄ trabajo_final/              # Proyecto integrador
‚îî‚îÄ‚îÄ docs/                       # Sitio web (MkDocs)
```

---

## Galer√≠a de Dashboards

| Dashboard | Tecnolog√≠as |
|-----------|-------------|
| [**ARIMA PRO**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_arima_pro.html) | Series temporales estilo Bloomberg |
| [**PCA + K-Means**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/02_pca_iris_dashboard.html) | Clustering y reducci√≥n dimensional |
| [**Transfer Learning Flores**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_flores.html) | CNN + Computer Vision |
| [**Panel Data QoG**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/06_analisis_panel_qog.html) | Spark + PostgreSQL + ML |
| [**Sismos Global**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_sismos_global.html) | Kafka + Tiempo Real |
| [**ISS Tracker**](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_iss_tracker.html) | LocalStack + AWS |

---

## Instructor

**Juan Marcelo Gutierrez Miranda** ‚Äî [@TodoEconometria](https://www.linkedin.com/in/juangutierrezconsultor/)

10+ a√±os en an√°lisis de datos y Big Data. Formador de profesionales en toda Latinoam√©rica y Espa√±a.

**Contacto:**
- Email: cursos@todoeconometria.com
- LinkedIn: [Juan Gutierrez](https://www.linkedin.com/in/juangutierrezconsultor/)
- Web: [todoeconometria.com](https://www.todoeconometria.com)

---

## Referencias Acad√©micas

1. **Dean, J., & Ghemawat, S. (2008).** MapReduce: Simplified data processing on large clusters. *Communications of the ACM*.
2. **Zaharia, M., et al. (2016).** Apache Spark: A unified engine for big data processing. *Communications of the ACM*.
3. **McKinney, W. (2022).** *Python for Data Analysis*. O'Reilly Media.
4. **Kleppmann, M. (2017).** *Designing Data-Intensive Applications*. O'Reilly Media.

---

---

## English

> **This course is available in English!** Visit the [English version of the website](https://todoeconometria.github.io/ejercicios-bigdata/en/).

A **complete, free, open-source** Big Data course covering 230 hours of hands-on content across 9 modules:

- **Databases:** SQLite, PostgreSQL, Oracle, DynamoDB
- **ETL & Processing:** Pandas, Dask, Apache Spark
- **Streaming:** Apache Kafka (KRaft mode), Spark Structured Streaming
- **Cloud:** LocalStack (free AWS simulation), Terraform, Lambda
- **ML & AI:** PCA, K-Means, TensorFlow, ARIMA/SARIMA, Transfer Learning
- **NLP:** NLTK, TF-IDF, Sentiment Analysis, Jaccard Similarity
- **Econometrics:** Panel Data, Fixed/Random Effects, Hausman Test
- **Infrastructure:** Docker, Docker Compose, Spark Clusters

**Live demos:** [Global Earthquake Observatory](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_sismos_global.html) | [ISS Tracker](https://todoeconometria.github.io/ejercicios-bigdata/dashboards/dashboard_iss_tracker.html)

---

<div align="center">

**¬© 2026 Juan Marcelo Gutierrez Miranda** ‚Äî Open Educational Material (MIT License)

**Hash ID:** 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c

</div>
